{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4b2b1c",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e344e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d98fa7",
   "metadata": {},
   "source": [
    "\n",
    "<b>Natural Language Processing</b>, or NLP, is a field of Artificial Intelligence that gives the machines the ability to read, understand, and derive meaning from human languages. NLP is a discipline where computer science, artificial intelligence, and cognitive logic are intercepted, with the objective to read, decipher, understand, and make sense of the human language in a valuable way.\n",
    "\n",
    "<u>NLP techniques are used to solve a variety of tasks such as</u>:\n",
    "\n",
    "* <b>Text Classification</b>: Categorizing text into predefined categories.\n",
    "* <b>Sentiment Analysis</b>: Determining the sentiment or opinion of a piece of text, like a review or a tweet.\n",
    "* <b>Information Extraction</b>: Extracting structured information from unstructured documents.\n",
    "* <b>Machine Translation</b>: Automatically translating text from one language to another.\n",
    "* <b>Topic Modeling</b>: Automatically discovering the abstract \"topics\" that occur in a collection of documents.\n",
    "\n",
    "    \n",
    " One of the most exciting aspects of NLP is the extraction of \"meaning\" from text, and a lot of NLP techniques revolve around this idea. In this notebook, we're going to focus on a specific technique called topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e31e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/nlp1.png\")\n",
    "# source: https://revolveai.com/natural-language-processing-techniques/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884716f",
   "metadata": {},
   "source": [
    "## Introduction to Python and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb96a5",
   "metadata": {},
   "source": [
    "Python is widely used in the field of Natural Language Processing (NLP) due to its simplicity and the  libraries that it provides for text processing and manipulation.\n",
    "\n",
    "Some of the most commonly used Python libraries for NLP are:\n",
    "* NLTK (Natural Language Toolkit): NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources.\n",
    "   - https://www.nltk.org/\n",
    "\n",
    "* spaCy: spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research and was designed from day one to be used in real products.\n",
    "   - https://spacy.io/universe/category/books\n",
    "   \n",
    "* Gensim: Gensim is a Python library for topic modelling, document indexing, and similarity retrieval with large corpora.\n",
    "  - https://github.com/RaRe-Technologies/gensim\n",
    "  \n",
    "  \n",
    "* Scikit-learn: While not a dedicated NLP library, Scikit-learn provides a lot of functions that are useful for text processing, such as feature extraction from text with CountVectorizer and TfidfVectorizer.\n",
    "  - https://scikit-learn.org/stable/\n",
    "\n",
    "* Transformers (from Hugging Face): This library provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, etc) for Natural Language Understanding (NLU) and Natural Language Generation (NLG).\n",
    "  - https://huggingface.co/docs/transformers/index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060ea64",
   "metadata": {},
   "source": [
    "In this notebook, we're going to be dealing with Topic Modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb76c4",
   "metadata": {},
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09849dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/TM.png\")\n",
    "# source: https://www.analyticsvidhya.com/blog/2021/07/topic-modelling-with-lda-a-hands-on-introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b88f96",
   "metadata": {},
   "source": [
    "Topic modeling is a type of statistical modeling for discovering abstract topics that occur in a collection of documents. At its core, topic modeling is about uncovering hidden structure in text data, and it is a powerful tool for organizing and understanding large collections of unstructured data.\n",
    "\n",
    "Topic models provide a simple way to analyze large volumes of unlabeled text. A \"topic\" consists of a cluster of words that frequently occur together. Using contextual clues, topic models can connect words with similar meanings and distinguish between uses of words with multiple meanings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed08fa4",
   "metadata": {},
   "source": [
    "### Topic modelling in the context of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ca7420",
   "metadata": {},
   "source": [
    "Topic modeling is an <b>unsupervised</b> machine learning task. The aim of topic modeling is to identify the main topics that occur in a collection of documents. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics.\n",
    "\n",
    "The reason it's an unsupervised task is because we don't know in advance what the topics are or how many topics there are. There's no \"correct\" answer that we're trying to predict. Instead, the algorithm tries to find patterns in the data and uses these patterns to determine the topics.\n",
    "\n",
    "There are several algorithms for topic modeling, including Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and others. Each of these algorithms has its own assumptions and methods for determining the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/tm2.png\")\n",
    "# source: https://www.cognub.com/index.php/cognitive-platform/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44852f36",
   "metadata": {},
   "source": [
    "### The practical part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec459e",
   "metadata": {},
   "source": [
    "Before we start with any Machine Learning or Natural Language Processing, we need data. Here, we are using the BBC News dataset. It contains articles from BBC News.\n",
    "\n",
    "In Python, we can use the pandas library to load the data from a csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fba1bd",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a5c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bbc-text.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088b127c",
   "metadata": {},
   "source": [
    "#### Check out the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c669793",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431a45e",
   "metadata": {},
   "source": [
    "LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. In the context of topic modeling, the \"observations\" are words in documents and the \"unobserved groups\" are topics. The \"why some parts of the data are similar\" is because similar documents share topics.\n",
    "\n",
    "Basically, it’s a way of explaining why some documents are similar to others (they are about the same topics). It uses a probabilistic graphical model where each document is assumed to be a mixture of various topics, and each word is probabilistically drawn from one topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/lda.png\")\n",
    "# Blei, D.M. (2012) Probabilistic Topic Models. Communications of the ACM, 55, 77-84. http://dx.doi.org/10.1145/2133806.2133826\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8aa7d",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f08828",
   "metadata": {},
   "source": [
    "The first step in any NLP task is text preprocessing. This usually involves converting all the text to lowercase, removing punctuation and stop words, and tokenization (breaking the text down into individual words).\n",
    "\n",
    "We're also going to do some extra steps specific to topic modelling: lemmatization, which reduces words to their root form. For instance, \"running\" would be reduced to \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed41d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0eb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    # Tokenization\n",
    "    text = text.split()\n",
    "    # Remove stop words and Lemmatize\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]\n",
    "    # Join words to a single string\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e78762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b132d",
   "metadata": {},
   "source": [
    "#### Digression - Lambda function:\n",
    " \n",
    " - An anonymous function in Python is one that has no name when it is defined. In Python, the lambda keyword is used to define anonymous functions rather than the def keyword, which is used for normal functions. As a result, lambda functions are another name for anonymous functions.\n",
    "\n",
    " -  here, x is the argument (an individual value from the 'text' column), and preprocess_text(x) is the expression. This lambda function applies the preprocess_text function to each value in the 'text' column.\n",
    "\n",
    "- so, the entire line of code is taking each value in the 'text' column of the dataframe, applying the preprocess_text function to it, and then storing the result in a new column called 'processed_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de32e0",
   "metadata": {},
   "source": [
    "Now that we have our text preprocessed, let's start with our first topic modelling algorithm: Latent Dirichlet Allocation (LDA). LDA assumes that every document is a mixture of topics and that every word in the document is attributable to the document's topics.\n",
    "\n",
    "We'll use the LDA implementation from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b144978",
   "metadata": {},
   "source": [
    "### Create a CountVectorizer for parsing/counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "term_matrix = count_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66881c0f",
   "metadata": {},
   "source": [
    "### Digression - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37354d2",
   "metadata": {},
   "source": [
    "- CountVectorizer is a class provided by the sklearn.feature_extraction.text module in the scikit-learn library. It's used to convert a collection of text documents to a matrix of token (word) counts.\n",
    "\n",
    "<b>When using CountVectorizer, the process usually involves the following steps:</b>\n",
    "\n",
    "1. The text is tokenized, meaning it's split into individual words according to some rule. By default, this is done by splitting the text on whitespace and punctuation.\n",
    "\n",
    "2. The words are counted. For each document, CountVectorizer maintains a count of how many times each word appeared.\n",
    "\n",
    "3. A document-term matrix is created. Each row of the matrix represents a document, and each column represents a unique word from across all documents. The entry in the ith row and the jth column of the matrix is the count of word j in document i.\n",
    "\n",
    "4. The output is a sparse matrix representation of the documents, which can be used as input to a machine learning model.\n",
    "\n",
    "#### max_df: \n",
    "- used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "- For example, max_df=0.95 means \"ignore terms that appear in more than 95% of the documents\"\n",
    "- These common words usually don't carry important meaning and are often removed.\n",
    "\n",
    "\n",
    "#### min_df:\n",
    "- used for removing terms that appear too infrequently\n",
    "- For example, min_df=2 means \"ignore terms that appear in less than 2 documents\"\n",
    "- The intuition behind this approach is that words that appear only once or a few times in the entire corpus might be typos, rare words or otherwise irrelevant to the analysis, thus can be safely ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c008a0",
   "metadata": {},
   "source": [
    "### Create and fit the LDA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc487cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57feb1de",
   "metadata": {},
   "source": [
    "In this section, the n_components stands for the number of topics we want to retreive. Playing with this parameter may also influence the quality of our results, since we are talking about clustering. Usually the best thing is to play around until we get a good evaluation score - or we simply figure that out topics look nice enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topics from the model\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic \", idx, \" \".join(count_vectorizer.get_feature_names()[i] for i in topic.argsort()[:-10 - 1:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935973aa",
   "metadata": {},
   "source": [
    " - the argsort() function from numpy gets the indices that would sort the topic array. It returns the indices that would sort an array.\n",
    "\n",
    "-  [:-11:-1] slice is getting the last 10 values from the sorted indices in reverse order. This gives you the indices of the 10 highest values in the topic array, which are the top 10 words for that topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cad6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47019b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the feature names from count vectorizer\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Get the topics and their top 10 words for LDA\n",
    "lda_topics = [[(feature_names[i], topic[i]) for i in topic.argsort()[:-11:-1]] for topic in lda.components_]\n",
    "\n",
    "for i, topic in enumerate(lda_topics):\n",
    "    wc = WordCloud(background_color=\"white\", max_words=2000)\n",
    "    wc.generate_from_frequencies(dict(topic))\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f'Topic {i+1}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61800abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f26db",
   "metadata": {},
   "source": [
    "### How do we interpret these topics? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513855cc",
   "metadata": {},
   "source": [
    "1. Check the most significant words: Start by looking at the words with the highest weights in each topic. These are the words that are most representative of the topic according to the model.\n",
    "\n",
    "2. Understand the common theme: Try to find a common theme or category among these words. For instance, if the top words are \"doctor\", \"patient\", \"hospital\", and \"medicine\", then a good interpretation of the topic might be \"Healthcare\" or \"Medicine\".\n",
    "\n",
    "3. Use your domain knowledge: Your own understanding of the subject matter can be very useful in interpreting the topics. For instance, if you're analyzing news articles and one of the topics contains words like \"election\", \"votes\", \"candidate\", and \"campaign\", then you could interpret this as a \"Politics\" topic.\n",
    "\n",
    "4. Check the related documents: Another way to interpret the topics is to look at some documents that are heavily associated with each topic. By reading these documents, you might get a better understanding of what the topic represents.\n",
    "\n",
    "5. Keep in mind that topics are probabilistic: Topic modelling algorithms like LDA are probabilistic, which means that they provide a probability distribution over all words for each topic, and a probability distribution over all topics for each document. The topics and the document-topic associations are not definitive but rather represent the algorithm's best guess based on the data and its own internal mathematics.\n",
    "\n",
    "6. Don't overinterpret: Finally, remember that not all topics might make perfect sense, and that's okay. Topic models are statistical models that try to find structure in the data, but sometimes this structure doesn't map perfectly onto human interpretability.\n",
    "\n",
    "Interpreting topics from topic modelling is more of an art than a science, requiring a mix of understanding the model's output, using your own domain knowledge, and making sensible judgments.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeeff1a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise 1 - LDA</b>\n",
    "<p>\n",
    "<li>Think of the topic names for the identified words in the word clouds. What would be the common themes?</li>\n",
    "\n",
    "</p>\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ce7ec6",
   "metadata": {},
   "source": [
    "## Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f8bbd",
   "metadata": {},
   "source": [
    "The next algorithm we will look at is Non-Negative Matrix Factorization (NMF). NMF is a dimensionality reduction and clustering algorithm that assumes that all data and components are non-negative. It's a faster and simpler alternative to LDA that can yield good results when dealing with smaller datasets.\n",
    "\n",
    "\n",
    "It assumes that all data and components are non-negative. The intuition behind NMF is that it decomposes (or factorizes) the document-term matrix into two other matrices: one with dimensions number-of-documents by number-of-topics, and the other with dimensions number-of-topics by number-of-terms.\n",
    "\n",
    "The main difference between NMF and LDA is that while LDA uses a probabilistic approach (assuming documents are generated by a probabilistic process), NMF uses a linear algebra approach. This makes NMF faster and more scalable to larger datasets, and it doesn't require tuning as many hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd22c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/nmf.png\")\n",
    "# https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeac06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b394e2",
   "metadata": {},
   "source": [
    "### Create the tf-idf feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6cbbe0",
   "metadata": {},
   "source": [
    "### Digression (want to know more) - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12ffd2",
   "metadata": {},
   "source": [
    "As we know, Machine Learning algorithms work with numerical data.\n",
    "\n",
    "However, text data is a form of categorical data and it's usually in a format that is not directly understandable by machine learning algorithms. Therefore, we need to convert these texts into some form of numerical representations for our machine learning models to understand and learn from.\n",
    "\n",
    "This process is called vectorization. There are multiple ways to convert text into vectors, some of which include:\n",
    "\n",
    "- Bag of Words (BoW): In this approach, each unique word in the text is represented by one number. It's the simplest and most intuitive way to convert text to numbers. Each document is represented as a vector in a multidimensional space, where each dimension corresponds to a term in the dataset's vocabulary.\n",
    "\n",
    "- TF-IDF (Term Frequency-Inverse Document Frequency): This is an improvement over BoW. TF-IDF also creates a document-term matrix, but it weights the counts by a measure of how unique the word is to the given document versus the entire corpus of documents. This helps to give higher weight to the more \"important\" words.\n",
    "\n",
    "- Word Embeddings (Word2Vec, GloVe): These are more advanced methods that represent words in a continuous vector space where semantically similar words are mapped to nearby points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465df61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd8f0ce",
   "metadata": {},
   "source": [
    "### Create and fit the NMF model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a598c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da75028",
   "metadata": {},
   "source": [
    "###  Display the topics from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a3d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in enumerate(nmf.components_):\n",
    "    print (\"Topic \", idx, \" \".join(tfidf_vectorizer.get_feature_names()[i] for i in topic.argsort()[:-10 - 1:-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787c7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the feature names from tfidf vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Get the topics and their top 10 words for NMF\n",
    "nmf_topics = [[(feature_names[i], topic[i]) for i in topic.argsort()[:-11:-1]] for topic in nmf.components_]\n",
    "\n",
    "for i, topic in enumerate(nmf_topics):\n",
    "    wc = WordCloud(background_color=\"white\", max_words=2000)\n",
    "    wc.generate_from_frequencies(dict(topic))\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f'Topic {i+1}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5408bc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise 2 - NMF</b>\n",
    "<p>\n",
    "<li>Compare these word clouds to the ones generated by LDA. What are the differences?</li>\n",
    "\n",
    "</p>\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7d7d3",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cf1f1b",
   "metadata": {},
   "source": [
    "To compare the quality of topics generated by LDA and NMF, we can use a measure called Topic Coherence. Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07022bf6",
   "metadata": {},
   "source": [
    "Coherence score is a widely used metric for evaluating the quality of the topics learned by a topic model. Higher coherence scores indicate that the top words in each topic are more semantically related, and therefore the topics are of better quality and more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea333700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae1efc",
   "metadata": {},
   "source": [
    "#### Create a dictionary representation of the documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aba02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(df['processed_text'].map(lambda x: x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8064e7b",
   "metadata": {},
   "source": [
    "We are creating a gensim dictionary object from our preprocessed text data. This dictionary is required for the calculation of coherence scores for the trained topic models. In gensim, a dictionary is a mapping between words and their integer ids. These dictionaries are used to create a 'bag-of-words' corpus in gensim, where each document is represented as a list of pairs of a word’s integer id and the word's frequency in the document.\n",
    "\n",
    "When calculating the coherence score for a topic model, gensim's CoherenceModel uses this dictionary and the original text data (split into individual words) to measure the degree of semantic similarity between high scoring words in the topic. That's why we need this dictionary for calculating the coherence score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290c9c5",
   "metadata": {},
   "source": [
    "#### Get the feature names from count vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b678119",
   "metadata": {},
   "source": [
    "get_feature_names() is a method of CountVectorizer that returns a list where each element is a string representing a feature (in this case, a word). The order of the features in the list corresponds to the column order in the output matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7412e",
   "metadata": {},
   "source": [
    "#### Get the topics from each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d20a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = [[feature_names[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda.components_]\n",
    "nmf_topics = [[feature_names[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in nmf.components_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a931490",
   "metadata": {},
   "source": [
    "- lda_topics and nmf_topics are a list of lists, where each sub-list represents a topic, and the elements of each sub-list are the top 10 words in the corresponding topic\n",
    "- the word is determined by mapping the index i to feature_names[i]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46696d",
   "metadata": {},
   "source": [
    "#### Get coherence for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052bf96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_coherence = CoherenceModel(topics=lda_topics, texts=df['processed_text'].map(lambda x: x.split()), dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "nmf_coherence = CoherenceModel(topics=nmf_topics, texts=df['processed_text'].map(lambda x: x.split()), dictionary=dictionary, coherence='c_v').get_coherence()\n",
    "\n",
    "print('LDA Coherence: ', lda_coherence)\n",
    "print('NMF Coherence: ', nmf_coherence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde27835",
   "metadata": {},
   "source": [
    "However, remember that interpretability of topics is subjective and dependent on the specific application or use case. Thus, while the coherence score serves as a decent measure of topic quality, you should also manually examine some of the topics and see how sensible they are. Furthermore, the best model for your application may also depend on factors like training time, which can vary significantly between these models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d313ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b1ce50",
   "metadata": {},
   "source": [
    "#### Get the feature names from count vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d86c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549fc435",
   "metadata": {},
   "source": [
    "#### Get the topics and their top 10 words for LDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics = [[(feature_names[i], topic[i]) for i in topic.argsort()[:-11:-1]] for topic in lda.components_]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5344e5",
   "metadata": {},
   "source": [
    "-  lda_topics is  a list of lists, where each sub-list represents a topic\n",
    "- However, in this case, the elements of each sub-list are tuples\n",
    "- Each tuple contains a word and its corresponding weight in the topic. The word is determined by mapping the index i to feature_names[i], and the weight is topic[i].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2018f6",
   "metadata": {},
   "source": [
    "topic.argsort()[:-10 - 1:-1] gets the indices of the top 10 words in each topic (by importance), and [feature_names[i] for i in ...] uses these indices to get the corresponding words. These lists of words are the topics that the CoherenceModel expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f540bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(lda_topics):\n",
    "    labels, values = zip(*topic)\n",
    "    fig = go.Figure(data=[go.Bar(x=values, y=labels, orientation='h')])\n",
    "    fig.update_layout(title_text=f'LDA Topic {i+1}')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cecea2",
   "metadata": {},
   "source": [
    "#### Get the topics and their top 10 words for NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd760b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topics = [[(feature_names[i], topic[i]) for i in topic.argsort()[:-11:-1]] for topic in nmf.components_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94142dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, topic in enumerate(nmf_topics):\n",
    "    labels, values = zip(*topic)\n",
    "    fig = go.Figure(data=[go.Bar(x=values, y=labels, orientation='h')])\n",
    "    fig.update_layout(title_text=f'NMF Topic {i+1}')\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e435d53",
   "metadata": {},
   "source": [
    "### Literature and references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa3ff4",
   "metadata": {},
   "source": [
    "- \"Latent Dirichlet Allocation\" by David M. Blei, Andrew Y. Ng, Michael I. Jordan (https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) - This is the original paper that introduced LDA.\n",
    "\n",
    "- Topic modeling in Python with NLTK and Gensim (https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) - This blog post provides a practical guide to topic modeling in Python.\n",
    "\n",
    "- Nonnegative Matrix Factorization (NMF) (https://medium.com/python-in-plain-english/topic-modelling-with-nmf-in-python-194eb6ae04a5) - Practical Guide - This post explains how to perform topic modeling using NMF with practical examples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
