{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d07ff02",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29661c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c119c",
   "metadata": {},
   "source": [
    "- the identification and classification of feelings, emotions, attitudes and opinions in text, is a rapidly growing research topic in various fields, e.g. digital humanities,  computer science, marketing, investment, business,...\n",
    "- also known as opinion mining\n",
    "- allows organizations to identify public sentiment towards certain words or topics to understand their users better\n",
    "- under the umbrella of sentiment analysis, we understand both emotion and polarity analysis, depending on the categories classified\n",
    "\n",
    "\n",
    "## Polarity analysis\n",
    "- classifying texts into three categories - positive, neutral and negative polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e41269",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/sentiment.jpg\")\n",
    "# source: https://www.expressanalytics.com/wp-content/uploads/2021/06/sentimentanalysishotelgeneric-2048x803-1.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ea878",
   "metadata": {},
   "source": [
    "## Emotion detection \n",
    "- emotion models established in the field of psychology (such as Paul Ekman's, Robert Plutchik's and Russell's models) - categories like sadness, joy, disgust, surprise, anger,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd9973",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/emotion.png\")\n",
    "# source: hthttps://clevertap.com/blog/how-emotions-incite-response/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3623fb9",
   "metadata": {},
   "source": [
    "## Sentiment analysis in the Digital Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e2783",
   "metadata": {},
   "source": [
    "Numerous applications:\n",
    "* the analysis of periodicals (Koncar et al 2022)\n",
    "* novels (Stanković et al 2022)\n",
    "* plays (Schmidt et al 2021)\n",
    "* poems (Sprugnoli et al 2022)\n",
    "* fairy tales (Zehe et al 2017)\n",
    "* song lyrics (Hernández-Lorenzo et al 2022)\n",
    "* Holocaust testimonies (Blanke et al 2020)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afebf9cc",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fada60f",
   "metadata": {},
   "source": [
    "## Lexicon-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca35eb",
   "metadata": {},
   "source": [
    "* A Sentiment Analysis Tool Chain for 18th Century Periodicals https://www.melusinapress.lu/read/ezpg-wk34/section/01ef33d1-8d9d-4391-a488-7d090f719858 oder https://gitlab.uni.lu/melusina/vdhd/koncar_sentiment \n",
    "* SentText https://thomasschmidtur.pythonanywhere.com \n",
    "* Syuzhet https://github.com/mjockers/syuzhet \n",
    "* VADER https://github.com/cjhutto/vaderSentiment \n",
    "* NRC Emotion Lexicon (EmoLex) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm \n",
    "* SentiWordNet 3.0 https://github.com/aesuli/SentiWordNet \n",
    "* SentiWS http://wortschatz.uni-leipzig.de/de/download "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d3e120",
   "metadata": {},
   "source": [
    "-  specialized sentiment lexicons were frequently used and often manually created by domain experts in the DH\n",
    "-  Sentiment lexicons or dictionaries are collections of words and phrases annotated with sentiment scores \n",
    "- lexicon-based methods do not require any training data and can be a good starting point for sentiment analysis tasks\n",
    "- however, their performance might not be as good as ML methods when it comes to understanding the context and handling complex sentences\n",
    "-  due to the large variety in style and dating of DH texts, these resources often need to be tailor-made for a specific    project, thus leading to an expensive and time-consuming process\n",
    "- alternative approach would be using machine learning and deep learning methods.\n",
    "- the choice of method depends on the specific requirements and the resources available at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b513e503",
   "metadata": {},
   "source": [
    "## ML-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99243687",
   "metadata": {},
   "source": [
    "- a supervised classification task\n",
    " \n",
    "#### Suggested models:\n",
    "\n",
    "### 1. Logistic Regression\n",
    "- simple yet effective machine learning algorithm that can be used for sentiment analysis\n",
    "- since it is classification, the dependent variable is  a discrete categorical one (e.g. if we want to predict a new customer vs returning one)\n",
    "- this algorithm is often used to predict between two discrete classes, e.g. pregnant and not pregnant or in our case now positive, newgative, neutral categories\n",
    "- works well with small and very clean datasets (no outliers and messy relationships, no missing values)\n",
    "- using bag-of-words or TF-IDF representation of a text, it estimates the probabilities of a particular text belonging to a specific sentiment category\n",
    "-  often performs well in text classification tasks\n",
    "- faster and easier to interpret compared to more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891eb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/logistic.png\",width=500, height=1500)\n",
    "# source: Machine Learning for Absolute Beginners, Oliver Theobald"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190453a1",
   "metadata": {},
   "source": [
    "#### EXAMPLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f722891f",
   "metadata": {},
   "source": [
    "Imagine we have two sentences:\n",
    "\n",
    "- \"I love this movie. It's fantastic.\"\n",
    "- \"I hate this movie. It's terrible.\"\n",
    "\n",
    "We first need to convert these sentences into numerical vectors. \n",
    "We could use a \"Bag of Words\" (BoW) model, Term Frequency-Inverse Document Frequency (TF-IDF), or even word embeddings.\n",
    "\n",
    "Let's say we use a simple BoW model and have transformed our sentences into vectors based on the frequency of words, resulting in vectors in a 3D space where each dimension corresponds to the words \"love\", \"fantastic\", and \"terrible\".\n",
    "\n",
    "Now our sentences might be represented as points in this 3-dimensional space.\n",
    "\n",
    "Let's say our positive sentence (\"I love this movie. It's fantastic.\") is represented as the point (1, 1, 0) because it contains the words \"love\" and \"fantastic\" once each and does not contain the word \"terrible\". Similarly, our negative sentence (\"I hate this movie. It's terrible.\") is represented as the point (0, 0, 1).\n",
    "\n",
    "The logistic regression model will find the \"decision boundary\", a hyperplane (in this case, a line in the 3D space) where points on one side belong to one category (\"positive\") and points on the other side belong to the other category (\"negative\").\n",
    "\n",
    "The logistic regression function is trained to calculate the probability that a given point belongs to the \"positive\" category. If the probability is greater than a certain threshold (commonly 0.5), it assigns the point to the \"positive\" category; otherwise, it assigns it to the \"negative\" category.\n",
    "\n",
    "Given a new sentence, we would convert it into the same 3D space. The logistic regression model will then calculate the probability that the new point belongs to the \"positive\" category, and assigns it a sentiment based on the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134aea5c",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes\n",
    "- commonly used algorithm for sentiment analysis\n",
    "- a probabilistic classifier that makes use of Bayes' Theorem, and it assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\n",
    "- this naive assumption of independence allows for simplicity and speed, which is a big reason why Naive Bayes is popular for text classification\n",
    "- particularly good at dealing with large feature spaces, which are common in text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7852d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/bayes.png\",width=500, height=1500)\n",
    "# https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0a8428",
   "metadata": {},
   "source": [
    "#### EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe657f1f",
   "metadata": {},
   "source": [
    "<U>FORMULA</U> \n",
    "\n",
    "Suppose we have a sentence \"I love this movie\". We want to classify whether the sentiment of this sentence is positive or negative.\n",
    "\n",
    "We have two classes:\n",
    "\n",
    "- A = Positive sentiment\n",
    "- B = Negative sentiment\n",
    "\n",
    "\n",
    "And we have four words in our sentence:\n",
    "\n",
    "- W1 = \"I\"\n",
    "- W2 = \"love\"\n",
    "- W3 = \"this\"\n",
    "- W4 = \"movie\"\n",
    "\n",
    "We want to calculate the probability that the sentiment is positive given these four words. According to Bayes' theorem, this can be represented as:\n",
    "\n",
    "```P(Positive | W1, W2, W3, W4)```\n",
    "\n",
    "\n",
    "\n",
    "The Naive Bayes classifier simplifies this by assuming that the words are independent of each other. While this is a naive assumption in most natural language cases, hence the name \"naive\", it often works well in practice.\n",
    "\n",
    "Thus, we can represent it as:\n",
    "\n",
    "```P(Positive | W1, W2, W3, W4) = P(W1 | Positive) * P(W2 | Positive) * P(W3 | Positive) * P(W4 | Positive) * P(Positive)```\n",
    "\n",
    "Here, the\n",
    "\n",
    "```P(W1 | Positive)``` is the probability of the word \"I\" appearing in a positive review.\n",
    "```P(W2 | Positive)``` is the probability of the word \"love\" appearing in a positive review, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd45f24",
   "metadata": {},
   "source": [
    "### 3. SVM\n",
    "- a powerful machine learning algorithm used for classification tasks, including sentiment analysis\n",
    "- works by finding a hyperplane in a N-dimensional space that distinctly classifies the data points\n",
    "- it handles high dimensional data well and is effective when there is a clear margin of separation between classes \n",
    "- SVMs can be computationally expensive and may not perform as well when the classes are highly overlapping\n",
    "- tend to be less interpretable than simpler models like Logistic Regression or Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"../img/svm_logistic.png\",width=500, height=1500)\n",
    "# source: Machine Learning for Absolute Beginners, Oliver Theobald"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca0708",
   "metadata": {},
   "source": [
    " ##### EXAMPLE\n",
    " \n",
    "- The idea of SVMs is simple -  The algorithm creates a line (or a hyperplane in higher dimensions) which separates the data into classes\n",
    "- The goal of SVM is to find the maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
    "\n",
    "\n",
    "Imagine we have two sentences:\n",
    "\n",
    "- \"I love this movie. It's fantastic.\"\n",
    "- \"I hate this movie. It's terrible.\"\n",
    "\n",
    "We need to convert these sentences into a form SVM can understand, i.e., numerical vectors\n",
    "One common way of doing this is by using a \"Bag of Words\" (BoW) model, which transforms the sentences into vectors based on the frequency of words.\n",
    "\n",
    "Let's say after using a BoW model, we have transformed our sentences into vectors in a 3D space where each dimension corresponds to the words \"love\", \"fantastic\", and \"terrible\", and the value in each dimension is the frequency of that word in the sentence.\n",
    "\n",
    "Now our sentences might be represented as points in this 3-dimensional space.\n",
    "\n",
    "Let's say our positive sentence (\"I love this movie. It's fantastic.\") is represented as the point (1, 1, 0) because it contains the words \"love\" and \"fantastic\" once each and does not contain the word \"terrible\". Similarly, our negative sentence (\"I hate this movie. It's terrible.\") is represented as the point (0, 0, 1).\n",
    "\n",
    "The SVM algorithm will find a hyperplane (in this case, a line in 3D space) that best separates these points based on their class. This hyperplane aims to maximize the margin, which is the distance between the hyperplane and the nearest point from either class.\n",
    "\n",
    "Now, given a new sentence, we would convert it to the same 3D space, and whichever side of the hyperplane it lands on is the class (positive or negative sentiment) that our SVM model predicts for the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a214bc",
   "metadata": {},
   "source": [
    "### Deep learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f264e2",
   "metadata": {},
   "source": [
    "-  state of the art for sentiment analysis\n",
    "- allow for more accurate classification and contextual understanding, but require large, well-annotated training corpora\n",
    "- the lack of annotated data to train such algorithms and the need for domain adaptation are the two main challenges faced by the DH community in applying these methods\n",
    "\n",
    "#### Models:\n",
    "- Recurrent Neural Network (RNN) - LSTMs and GRU\n",
    "- transformers  - BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer)\n",
    "\n",
    "\n",
    "- transformers are currently outpreforming all else, due to their ability to understand the context of words in all positions of the text\n",
    "- however very computationally intensive, require a lot of labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9123575d",
   "metadata": {},
   "source": [
    "### Comparison of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e4a76",
   "metadata": {},
   "source": [
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Logistic Regression | Simple to understand and implement. Less prone to overfitting. | Assumption of linearity. Might not work well with a large number of features. |\n",
    "| Naive Bayes | Fast and easy to implement. Performs well in text classification tasks. | Assumption of independent predictors. In real-life, it's almost impossible that we get a set of predictors which are completely independent. |\n",
    "| SVM | Effective in high dimensional spaces. Works well with clear margin of separation. | Not suitable for large datasets. Does not perform well when classes are overlapped. |\n",
    "| Lexicon-based | Does not require any training data. Takes into account the intensity of sentiment. | Can't understand the context. Fails to handle complex sentences. |\n",
    "| Deep Learning (BERT, GPT etc.) | Can handle complex language tasks. Does not require feature engineering. | Requires a lot of computational resources. Black box model. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b63fa",
   "metadata": {},
   "source": [
    "## Let's get going - sentiment analysis with a lexicon + shallow ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb21cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac034c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75370ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e3c7b5",
   "metadata": {},
   "source": [
    "## 1. Vader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6156f45",
   "metadata": {},
   "source": [
    "### Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/FinancialPhraseBank.csv', delimiter=',', header=None, encoding= 'latin-1')\n",
    "df.columns = ['Sentiment', 'Text']\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc9262",
   "metadata": {},
   "source": [
    "Let's apply the Vader sentiment analyzer on our dataset. For this, we'll use the NLTK library's Vader module.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03099906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(dataframe):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    dataframe['compound'] = dataframe['Text'].apply(lambda x: sid_obj.polarity_scores(x)['compound'])\n",
    "    \n",
    "    def get_sentiment(compound):\n",
    "        if compound >= 0.05:\n",
    "            return \"positive\"\n",
    "        elif compound <= -0.05:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "        \n",
    "    dataframe['vader_sentiment'] = dataframe['compound'].apply(get_sentiment)\n",
    "    return dataframe\n",
    "\n",
    "df = sentiment_scores(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8206d1f9",
   "metadata": {},
   "source": [
    "You might ask why we didn't do any cleaning. That's because the VADER sentiment analysis tool is specifically tuned to  text data and it is very sensitive to both the syntax and semantics of the input text. It doesn't just rely on a list of individual sentiment-bearing words, but also understands how the words interact in the context of the sentence (through things like punctuation, capitalization, intensifiers, and word order).\n",
    "\n",
    "- VADER is case-sensitive. This means it understands the difference between words written in different cases (like \"GOOD\" and \"good\") and assigns different sentiment intensities to them. So, converting all the text to lowercase, a common step in data cleaning for many NLP tasks, is not advisable when using VADER.\n",
    "\n",
    "- Punctuation: Similarly, VADER also considers punctuation in determining the sentiment of the text. Exclamation points, for example, can intensify the sentiment of a statement. So, removing punctuation, another common data cleaning step, is also not recommended when using VADER.\n",
    "\n",
    "- Stop words: VADER already takes care of common words (stop words) and understands their sentiment implications in context. Therefore, it is not necessary to remove stop words from the text data when using VADER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab8ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = accuracy_score(df['Sentiment'], df['vader_sentiment'])\n",
    "precision = precision_score(df['Sentiment'], df['vader_sentiment'], average='macro')\n",
    "recall = recall_score(df['Sentiment'], df['vader_sentiment'], average='macro')\n",
    "f1 = f1_score(df['Sentiment'], df['vader_sentiment'], average='macro')\n",
    "\n",
    "print(f\"Vader accuracy: {accuracy}\")\n",
    "print(f\"vader precision: {precision}\")\n",
    "print(f\"Vader recall: {recall}\")\n",
    "print(f\"Vader F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d9061",
   "metadata": {},
   "source": [
    "- average='macro' argument calculates the metric independently for each class and then takes the average, which treats all classes equally\n",
    "- if your classes have imbalances, you might want to consider using average='weighted', which weights the metric by the number of samples of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c23761",
   "metadata": {},
   "source": [
    "## 2. ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Vectorize the Text column\n",
    "X = vectorizer.fit_transform(data['Text'])\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4470d0",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d20980",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(max_iter=1000)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0f986",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d910064",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd61cfa3",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = LinearSVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_pred_svm = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848dcc5",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c13359",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Metrics for Logistic Regression\n",
    "print('Metrics for Logistic Regression:')\n",
    "print_metrics(y_test, y_pred_lr)\n",
    "\n",
    "# Metrics for Naive Bayes\n",
    "print('\\nMetrics for Naive Bayes:')\n",
    "print_metrics(y_test, y_pred_nb)\n",
    "\n",
    "# Metrics for SVM\n",
    "print('\\nMetrics for SVM:')\n",
    "print_metrics(y_test, y_pred_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb075c",
   "metadata": {},
   "source": [
    "From all this we can conclude that Logistic regression worked best four our dataset, followed by SVM, Naive Bayes and the lexicon. \n",
    "\n",
    "### Why?\n",
    "-  Logistic regression is specifically designed for these kind of  binary classification problems so it models the probability that each input instance belongs to the positive class, which fits the nature of this task very well\n",
    "- Logistic regression does not require features to be independent, unlike Naive Bayes. This can be a significant advantage when dealing with text data, as words in a sentence are often not independent of each other\n",
    "- logistic regression does not require features to be independent (words), unlike Naive Bayes, which can be a significant advantage when dealing with text data, as words in a sentence are often not independent of each other\n",
    "- if we tuned the parameters of th SVM better (e.g. the C - penalty for missclassification), we might have gotten a better result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e1c7a",
   "metadata": {},
   "source": [
    "#### Further Reading\n",
    "\n",
    "<B>BOOKS</B>:\n",
    "- Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper\n",
    "- Speech and Language Processing by Daniel Jurafsky and James H. Martin\n",
    "- Deep Learning for Natural Language Processing by Palash Goyal, Sumit Pandey, and Karan Jain\n",
    "\n",
    "\n",
    "<B>ARTICLES</B>\n",
    "- Acheampong, Francisca Adoma; Wenyu, Chen; Nuuno-Mensah, Henry: Text‐based Emotion Detection: Advances, Challenges, and Opportunities. 2020. Engineering Reports, 2, 7. DOI: https://doi.org/10.1002/eng2.12189. [W6BG3BF5]\n",
    "- Dang, Nhan Cach; Moreno-García, Maria N.; De la Prieta, Fernando. Sentiment Analysis Based on Deep Learning: A Comparative Study.2020. Electronics, 9, 483. DOI: https://doi.org/10.3390/electronics9030483 [9M88FBQ9]\n",
    "- Blanke, Tobias; Brayant, Michel; Hedges, Mark:  Understanding Memories of the Holocaust—A New Approach to Neural Networks in the Digital Humanities. 2020. Digital Scholarship in the Humanities, 35, 1. Oxford University Press (OUP), 17–33. DOI: https://doi.org/10.1093/llc/fqy082 . [B4ETJDFD]\n",
    "- Hartmann, Jochen; Heitmann, Mark; Siebert, Christian; Schamp, Christina: More than a Feeling: Accuracy and Application of Sentiment Analysis. 2023. International Journal of Research in Marketing, 40,1, 75–87. DOI: https://doi.org/10.1016/j.ijresmar.2022.05.005. [J5E77P3M]\n",
    "- Hernández-Lorenzo, Laura; Diaz, Aitor; Perez, Alvaro; Ros, Salvador; Gonzalez-Bianco, Elena: Exploring Spanish contemporary song lyrics through Digital Humanities methods: Some thematic and structural properties. 2022. Digital Scholarship in the Humanities. 37, 3, 738–746. DOI: https://doi.org/10.1093/llc/fqab083 [4PRVFJNZ]\n",
    "- Koncar, Philipp; Geiger, Bernhard C.; Glatz, Christina; Hobisch, Elisabeth; Sarić, Sanja; Scholger, Martina; Völkl, Yvonne; Helic, Denis: A Sentiment Analysis Tool Chain for 18th Century Periodicals. 2022. In: Manuel Burghardt, Lisa Dieckmann, Timo Steyer, Peer Trilcke, Niels-Oliver Walkowski, Joëlle Weis, Ulrike Wuttke (eds.): Fabrikation von Erkenntnis. Experimente in den Digital Humanities. Luxembourg. Zeitschrift für digitale Geisteswissenschaften und Melusina Press. DOI: https://doi.org/10.26298/ezpg-wk34. [ZE7V6L5N]\n",
    "- Schmidt, Thomas; Dennerlein, Katrin; Wolff, Christian : Using Deep Learning for Emotion Analysis of 18th and 19th Century German Plays. 2021. In: Burghardt, Manuel and Dieckmann, Lisa and Steyer, Timo and Trilcke, Peer and Walkowski, Niels-Oliver and Weis, Joëlle and Wuttke, Ulrike, (eds.): Fabrikation von Erkenntnis: Experimente in den Digital Humanities. Teilband 1. Melusina Press, Esch-sur-Alzette, Luxembourg. ISBN:  978-2-919815-25-8. [RHVH9KPB]\n",
    "- Sprugnoli, Rachele; Passarotti, Marco; Cecchini, Flavio Massimiliano; Pellegrini, Matteo: Overview of the EvaLatin 2020 Evaluation Campaign. 2020. Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages (105–110). Marseille: European Language Resources Association (ELRA). [9HI22FW8]\n",
    "- Sprugnoli, Rachele;  Mambrini, Francesco; Passarotti, Marco; Moretti, Giovanni: Sentiment Analysis of Latin Poetry: First Experiments on the Odes of Horace. 2021. Proceedings of the Eighth Italian Conference on Computational Linguistics. Milan, Italy, 314–320. DOI:  https://doi.org/10.5281/zenodo.5773792. [7MNF9SMT]\n",
    "- Stanković, Ranka; Miloš Košprdić: Sentiment Analysis of Serbian Old Novels. 2022. Proceedings of the 2nd Workshop on Sentiment Analysis and Linguistic Linked Data, European Language Resources Association, 31–38. [TPHTTER9]\n",
    "- Suissa, Omri, et al. “Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science.” Journal of the Association for Information Science and Technology, vol. 73, no. 2, 2022, pp. 268–87, https://doi.org/10.1002/asi.24544. [6TAVZY7M]\n",
    "- Zehe, Albin; Becker, Martin; Jannidis, Fotis; Hotho, Andreas: Towards Sentiment Analysis on German Literature. 2017. In: Kern-Isberner, G., Fürnkranz, J., Thimm, M. (eds) KI 2017: Advances in Artificial Intelligence. KI 2017. Lecture Notes in Computer Science(), 10505. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-67190-1_36. [3AHVI274]\n",
    "\n",
    "<B>TUTORIALS</B>\n",
    "- https://huggingface.co/blog/sentiment-analysis-python\n",
    "- https://towardsdatascience.com/a-step-by-step-tutorial-for-conducting-sentiment-analysis-a7190a444366\n",
    "- https://realpython.com/python-nltk-sentiment-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
