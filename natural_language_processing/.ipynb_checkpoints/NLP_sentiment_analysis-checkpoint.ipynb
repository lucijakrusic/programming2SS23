{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a06c0401",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9be5b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0a858",
   "metadata": {},
   "source": [
    "- the identification and classification of feelings, emotions, attitudes and opinions in text, is a rapidly growing research topic in various fields, e.g. digital humanities,  computer science, marketing, investment, business,...\n",
    "- also known as opinion mining\n",
    "- allows organizations to identify public sentiment towards certain words or topics to understand their users better\n",
    "- under the umbrella of sentiment analysis, we understand both emotion and polarity analysis, depending on the categories classified\n",
    "\n",
    "\n",
    "## Polarity analysis\n",
    "- classifying texts into three categories - positive, neutral and negative polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a49eb3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/sentiment.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/sentiment.jpg\")\n",
    "# source: https://www.expressanalytics.com/wp-content/uploads/2021/06/sentimentanalysishotelgeneric-2048x803-1.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437ffb1",
   "metadata": {},
   "source": [
    "## Emotion detection \n",
    "- emotion models established in the field of psychology (such as Paul Ekman's, Robert Plutchik's and Russell's models) - categories like sadness, joy, disgust, surprise, anger,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c56b9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/emotion.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/emotion.png\")\n",
    "# source: hthttps://clevertap.com/blog/how-emotions-incite-response/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef43ffc8",
   "metadata": {},
   "source": [
    "## Sentiment analysis in the Digital Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0634256",
   "metadata": {},
   "source": [
    "Numerous applications:\n",
    "* the analysis of periodicals (Koncar et al 2022)\n",
    "* novels (Stanković et al 2022)\n",
    "* plays (Schmidt et al 2021)\n",
    "* poems (Sprugnoli et al 2022)\n",
    "* fairy tales (Zehe et al 2017)\n",
    "* song lyrics (Hernández-Lorenzo et al 2022)\n",
    "* Holocaust testimonies (Blanke et al 2020)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb9367",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dce5d",
   "metadata": {},
   "source": [
    "## Lexicon-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f083a35e",
   "metadata": {},
   "source": [
    "* A Sentiment Analysis Tool Chain for 18th Century Periodicals https://www.melusinapress.lu/read/ezpg-wk34/section/01ef33d1-8d9d-4391-a488-7d090f719858 oder https://gitlab.uni.lu/melusina/vdhd/koncar_sentiment \n",
    "* SentText https://thomasschmidtur.pythonanywhere.com \n",
    "* Syuzhet https://github.com/mjockers/syuzhet \n",
    "* VADER https://github.com/cjhutto/vaderSentiment \n",
    "* NRC Emotion Lexicon (EmoLex) https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm \n",
    "* SentiWordNet 3.0 https://github.com/aesuli/SentiWordNet \n",
    "* SentiWS http://wortschatz.uni-leipzig.de/de/download "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f677ba",
   "metadata": {},
   "source": [
    "-  specialized sentiment lexicons were frequently used and often manually created by domain experts in the DH\n",
    "-  Sentiment lexicons or dictionaries are collections of words and phrases annotated with sentiment scores \n",
    "- lexicon-based methods do not require any training data and can be a good starting point for sentiment analysis tasks\n",
    "- however, their performance might not be as good as ML methods when it comes to understanding the context and handling complex sentences\n",
    "-  due to the large variety in style and dating of DH texts, these resources often need to be tailor-made for a specific    project, thus leading to an expensive and time-consuming process\n",
    "- alternative approach would be using machine learning and deep learning methods.\n",
    "- the choice of method depends on the specific requirements and the resources available at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6448c6",
   "metadata": {},
   "source": [
    "## ML-based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375f571",
   "metadata": {},
   "source": [
    "- a supervised classification task\n",
    " \n",
    "#### Suggested models:\n",
    "\n",
    "### 1. Logistic Regression\n",
    "- simple yet effective machine learning algorithm that can be used for sentiment analysis\n",
    "- since it is classification, the dependent variable is  a discrete categorical one (e.g. if we want to predict a new customer vs returning one)\n",
    "- this algorithm is often used to predict between two discrete classes, e.g. pregnant and not pregnant or in our case now positive, newgative, neutral categories\n",
    "- works well with small and very clean datasets (no outliers and messy relationships, no missing values)\n",
    "- using bag-of-words or TF-IDF representation of a text, it estimates the probabilities of a particular text belonging to a specific sentiment category\n",
    "-  often performs well in text classification tasks\n",
    "- faster and easier to interpret compared to more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dceafafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/logistic.png\" width=\"500\" height=\"1500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/logistic.png\",width=500, height=1500)\n",
    "# source: Machine Learning for Absolute Beginners, Oliver Theobald"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c3a3c",
   "metadata": {},
   "source": [
    "#### EXAMPLE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e1d95",
   "metadata": {},
   "source": [
    "Imagine we have two sentences:\n",
    "\n",
    "- \"I love this movie. It's fantastic.\"\n",
    "- \"I hate this movie. It's terrible.\"\n",
    "\n",
    "We first need to convert these sentences into numerical vectors. \n",
    "We could use a \"Bag of Words\" (BoW) model, Term Frequency-Inverse Document Frequency (TF-IDF), or even word embeddings.\n",
    "\n",
    "Let's say we use a simple BoW model and have transformed our sentences into vectors based on the frequency of words, resulting in vectors in a 3D space where each dimension corresponds to the words \"love\", \"fantastic\", and \"terrible\".\n",
    "\n",
    "Now our sentences might be represented as points in this 3-dimensional space.\n",
    "\n",
    "Let's say our positive sentence (\"I love this movie. It's fantastic.\") is represented as the point (1, 1, 0) because it contains the words \"love\" and \"fantastic\" once each and does not contain the word \"terrible\". Similarly, our negative sentence (\"I hate this movie. It's terrible.\") is represented as the point (0, 0, 1).\n",
    "\n",
    "The logistic regression model will find the \"decision boundary\", a hyperplane (in this case, a line in the 3D space) where points on one side belong to one category (\"positive\") and points on the other side belong to the other category (\"negative\").\n",
    "\n",
    "The logistic regression function is trained to calculate the probability that a given point belongs to the \"positive\" category. If the probability is greater than a certain threshold (commonly 0.5), it assigns the point to the \"positive\" category; otherwise, it assigns it to the \"negative\" category.\n",
    "\n",
    "Given a new sentence, we would convert it into the same 3D space. The logistic regression model will then calculate the probability that the new point belongs to the \"positive\" category, and assigns it a sentiment based on the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382305bd",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes\n",
    "- commonly used algorithm for sentiment analysis\n",
    "- a probabilistic classifier that makes use of Bayes' Theorem, and it assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature\n",
    "- this naive assumption of independence allows for simplicity and speed, which is a big reason why Naive Bayes is popular for text classification\n",
    "- particularly good at dealing with large feature spaces, which are common in text data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "483cdb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/bayes.png\" width=\"500\" height=\"1500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/bayes.png\",width=500, height=1500)\n",
    "# https://medium.com/analytics-vidhya/na%C3%AFve-bayes-algorithm-5bf31e9032a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffae15",
   "metadata": {},
   "source": [
    "#### EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb2313",
   "metadata": {},
   "source": [
    "<U>FORMULA</U> \n",
    "\n",
    "Suppose we have a sentence \"I love this movie\". We want to classify whether the sentiment of this sentence is positive or negative.\n",
    "\n",
    "We have two classes:\n",
    "\n",
    "- A = Positive sentiment\n",
    "- B = Negative sentiment\n",
    "\n",
    "\n",
    "And we have four words in our sentence:\n",
    "\n",
    "- W1 = \"I\"\n",
    "- W2 = \"love\"\n",
    "- W3 = \"this\"\n",
    "- W4 = \"movie\"\n",
    "\n",
    "We want to calculate the probability that the sentiment is positive given these four words. According to Bayes' theorem, this can be represented as:\n",
    "\n",
    "```P(Positive | W1, W2, W3, W4)```\n",
    "\n",
    "\n",
    "\n",
    "The Naive Bayes classifier simplifies this by assuming that the words are independent of each other. While this is a naive assumption in most natural language cases, hence the name \"naive\", it often works well in practice.\n",
    "\n",
    "Thus, we can represent it as:\n",
    "\n",
    "```P(Positive | W1, W2, W3, W4) = P(W1 | Positive) * P(W2 | Positive) * P(W3 | Positive) * P(W4 | Positive) * P(Positive)```\n",
    "\n",
    "Here, the\n",
    "\n",
    "```P(W1 | Positive)``` is the probability of the word \"I\" appearing in a positive review.\n",
    "```P(W2 | Positive)``` is the probability of the word \"love\" appearing in a positive review, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72de49c0",
   "metadata": {},
   "source": [
    "### 3. SVM\n",
    "- a powerful machine learning algorithm used for classification tasks, including sentiment analysis\n",
    "- works by finding a hyperplane in a N-dimensional space that distinctly classifies the data points\n",
    "- it handles high dimensional data well and is effective when there is a clear margin of separation between classes \n",
    "- SVMs can be computationally expensive and may not perform as well when the classes are highly overlapping\n",
    "- tend to be less interpretable than simpler models like Logistic Regression or Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d338904f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/svm_logistic.png\" width=\"500\" height=\"1500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/svm_logistic.png\",width=500, height=1500)\n",
    "# source: Machine Learning for Absolute Beginners, Oliver Theobald"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf40bb",
   "metadata": {},
   "source": [
    " ##### EXAMPLE\n",
    " \n",
    "- The idea of SVMs is simple -  The algorithm creates a line (or a hyperplane in higher dimensions) which separates the data into classes\n",
    "- The goal of SVM is to find the maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n",
    "\n",
    "\n",
    "Imagine we have two sentences:\n",
    "\n",
    "- \"I love this movie. It's fantastic.\"\n",
    "- \"I hate this movie. It's terrible.\"\n",
    "\n",
    "We need to convert these sentences into a form SVM can understand, i.e., numerical vectors\n",
    "One common way of doing this is by using a \"Bag of Words\" (BoW) model, which transforms the sentences into vectors based on the frequency of words.\n",
    "\n",
    "Let's say after using a BoW model, we have transformed our sentences into vectors in a 3D space where each dimension corresponds to the words \"love\", \"fantastic\", and \"terrible\", and the value in each dimension is the frequency of that word in the sentence.\n",
    "\n",
    "Now our sentences might be represented as points in this 3-dimensional space.\n",
    "\n",
    "Let's say our positive sentence (\"I love this movie. It's fantastic.\") is represented as the point (1, 1, 0) because it contains the words \"love\" and \"fantastic\" once each and does not contain the word \"terrible\". Similarly, our negative sentence (\"I hate this movie. It's terrible.\") is represented as the point (0, 0, 1).\n",
    "\n",
    "The SVM algorithm will find a hyperplane (in this case, a line in 3D space) that best separates these points based on their class. This hyperplane aims to maximize the margin, which is the distance between the hyperplane and the nearest point from either class.\n",
    "\n",
    "Now, given a new sentence, we would convert it to the same 3D space, and whichever side of the hyperplane it lands on is the class (positive or negative sentiment) that our SVM model predicts for the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6176dd",
   "metadata": {},
   "source": [
    "### Deep learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa00ed8",
   "metadata": {},
   "source": [
    "-  state of the art for sentiment analysis\n",
    "- allow for more accurate classification and contextual understanding, but require large, well-annotated training corpora\n",
    "- the lack of annotated data to train such algorithms and the need for domain adaptation are the two main challenges faced by the DH community in applying these methods\n",
    "\n",
    "#### Models:\n",
    "- Recurrent Neural Network (RNN) - LSTMs and GRU\n",
    "- transformers  - BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer)\n",
    "\n",
    "\n",
    "- transformers are currently outpreforming all else, due to their ability to understand the context of words in all positions of the text\n",
    "- however very computationally intensive, require a lot of labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655364da",
   "metadata": {},
   "source": [
    "### Comparison of methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbf5cb",
   "metadata": {},
   "source": [
    "| Model | Pros | Cons |\n",
    "| --- | --- | --- |\n",
    "| Logistic Regression | Simple to understand and implement. Less prone to overfitting. | Assumption of linearity. Might not work well with a large number of features. |\n",
    "| Naive Bayes | Fast and easy to implement. Performs well in text classification tasks. | Assumption of independent predictors. In real-life, it's almost impossible that we get a set of predictors which are completely independent. |\n",
    "| SVM | Effective in high dimensional spaces. Works well with clear margin of separation. | Not suitable for large datasets. Does not perform well when classes are overlapped. |\n",
    "| Lexicon-based | Does not require any training data. Takes into account the intensity of sentiment. | Can't understand the context. Fails to handle complex sentences. |\n",
    "| Deep Learning (BERT, GPT etc.) | Can handle complex language tasks. Does not require feature engineering. | Requires a lot of computational resources. Black box model. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda87e3",
   "metadata": {},
   "source": [
    "## Let's get going - sentiment analysis with a lexicon + shallow ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c58194d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99f0ce9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\krusic\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "694f07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795bfdef",
   "metadata": {},
   "source": [
    "## 1. Vader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d5f5c",
   "metadata": {},
   "source": [
    "### Loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a261dada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>positive</td>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>positive</td>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                               Text\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f...\n",
       "5  positive  FINANCING OF ASPOCOMP 'S GROWTH Aspocomp is ag...\n",
       "6  positive  For the last quarter of 2010 , Componenta 's n...\n",
       "7  positive  In the third quarter of 2010 , net sales incre...\n",
       "8  positive  Operating profit rose to EUR 13.1 mn from EUR ...\n",
       "9  positive  Operating profit totalled EUR 21.1 mn , up fro..."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/FinancialPhraseBank.csv', delimiter=',', header=None, encoding= 'latin-1')\n",
    "df.columns = ['Sentiment', 'Text']\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7605b185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     2879\n",
       "positive    1363\n",
       "negative     604\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bb18c8",
   "metadata": {},
   "source": [
    "Let's apply the Vader sentiment analyzer on our dataset. For this, we'll use the NLTK library's Vader module.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ffc9bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Text</th>\n",
       "      <th>compound</th>\n",
       "      <th>vader_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>-0.1280</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>0.8555</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>negative</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>negative</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>negative</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>negative</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentiment                                               Text  compound  \\\n",
       "0      neutral  According to Gran , the company has no plans t...   -0.1280   \n",
       "1      neutral  Technopolis plans to develop in stages an area...   -0.2960   \n",
       "2     negative  The international electronic industry company ...    0.0000   \n",
       "3     positive  With the new production plant the company woul...    0.8555   \n",
       "4     positive  According to the company 's updated strategy f...    0.6705   \n",
       "...        ...                                                ...       ...   \n",
       "4841  negative  LONDON MarketWatch -- Share prices ended lower...   -0.7269   \n",
       "4842   neutral  Rinkuskiai 's beer sales fell by 6.5 per cent ...    0.0000   \n",
       "4843  negative  Operating profit fell to EUR 35.4 mn from EUR ...    0.7430   \n",
       "4844  negative  Net sales of the Paper segment decreased to EU...    0.4404   \n",
       "4845  negative  Sales in Finland decreased by 10.5 % in Januar...    0.0000   \n",
       "\n",
       "     vader_sentiment  \n",
       "0           negative  \n",
       "1           negative  \n",
       "2            neutral  \n",
       "3           positive  \n",
       "4           positive  \n",
       "...              ...  \n",
       "4841        negative  \n",
       "4842         neutral  \n",
       "4843        positive  \n",
       "4844        positive  \n",
       "4845         neutral  \n",
       "\n",
       "[4846 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment_scores(dataframe):\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    dataframe['compound'] = dataframe['Text'].apply(lambda x: sid_obj.polarity_scores(x)['compound'])\n",
    "    \n",
    "    def get_sentiment(compound):\n",
    "        if compound >= 0.05:\n",
    "            return \"positive\"\n",
    "        elif compound <= -0.05:\n",
    "            return \"negative\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "        \n",
    "    dataframe['vader_sentiment'] = dataframe['compound'].apply(get_sentiment)\n",
    "    return dataframe\n",
    "\n",
    "df = sentiment_scores(df)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ca21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3234a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader accuracy: 0.5429219975237309\n",
      "vader precision: 0.5162941568162106\n",
      "Vader recall: 0.5072584729716739\n",
      "Vader F1 Score: 0.48833005717003236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = accuracy_score(df['Sentiment'], df['vader_sentiment'])\n",
    "precision = precision_score(df['Sentiment'], df['vader_sentiment'], average='macro')\n",
    "recall = recall_score(df['Sentiment'], df['vader_sentiment'], average='macro')\n",
    "f1 = f1_score(df['Sentiment'], df['vader_sentiment'], average='macro')\n",
    "\n",
    "print(f\"Vader accuracy: {accuracy}\")\n",
    "print(f\"vader precision: {precision}\")\n",
    "print(f\"Vader recall: {recall}\")\n",
    "print(f\"Vader F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684efb88",
   "metadata": {},
   "source": [
    "- average='macro' argument calculates the metric independently for each class and then takes the average, which treats all classes equally\n",
    "- if your classes have imbalances, you might want to consider using average='weighted', which weights the metric by the number of samples of each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc7a2b8",
   "metadata": {},
   "source": [
    "## 2. ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc697638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5a0a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Vectorize the Text column\n",
    "X = vectorizer.fit_transform(data['Text'])\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['Sentiment'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b37234",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd93ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LogisticRegression(max_iter=1000)\n",
    "clf_lr.fit(X_train, y_train)\n",
    "y_pred_lr = clf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd2f0b9",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "276f2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "y_pred_nb = clf_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe729d99",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b9ed36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = LinearSVC()\n",
    "clf_svm.fit(X_train, y_train)\n",
    "y_pred_svm = clf_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6f095",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efeb833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "95e7224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Logistic Regression:\n",
      "Accuracy:  0.765979381443299\n",
      "Precision:  0.7639926068182412\n",
      "Recall:  0.765979381443299\n",
      "F1 Score:  0.7547329531068694\n",
      "\n",
      "Metrics for Naive Bayes:\n",
      "Accuracy:  0.709278350515464\n",
      "Precision:  0.7050781073987011\n",
      "Recall:  0.709278350515464\n",
      "F1 Score:  0.70650813302979\n",
      "\n",
      "Metrics for SVM:\n",
      "Accuracy:  0.7422680412371134\n",
      "Precision:  0.7365012906720486\n",
      "Recall:  0.7422680412371134\n",
      "F1 Score:  0.7366626098397215\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred, average='weighted'))\n",
    "    print('Recall: ', recall_score(y_test, y_pred, average='weighted'))\n",
    "    print('F1 Score: ', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Metrics for Logistic Regression\n",
    "print('Metrics for Logistic Regression:')\n",
    "print_metrics(y_test, y_pred_lr)\n",
    "\n",
    "# Metrics for Naive Bayes\n",
    "print('\\nMetrics for Naive Bayes:')\n",
    "print_metrics(y_test, y_pred_nb)\n",
    "\n",
    "# Metrics for SVM\n",
    "print('\\nMetrics for SVM:')\n",
    "print_metrics(y_test, y_pred_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4206f47",
   "metadata": {},
   "source": [
    "From all this we can conclude that Logistic regression worked best four our dataset, followed by SVM, Naive Bayes and the lexicon. \n",
    "\n",
    "### Why?\n",
    "-  Logistic regression is specifically designed for these kind of  binary classification problems so it models the probability that each input instance belongs to the positive class, which fits the nature of this task very well\n",
    "- Logistic regression does not require features to be independent, unlike Naive Bayes. This can be a significant advantage when dealing with text data, as words in a sentence are often not independent of each other\n",
    "- logistic regression does not require features to be independent (words), unlike Naive Bayes, which can be a significant advantage when dealing with text data, as words in a sentence are often not independent of each other\n",
    "- if we tuned the parameters of th SVM better (e.g. the C - penalty for missclassification), we might have gotten a better result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a850c3",
   "metadata": {},
   "source": [
    "#### Further Reading\n",
    "\n",
    "<B>BOOKS</B>:\n",
    "- Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper\n",
    "- Speech and Language Processing by Daniel Jurafsky and James H. Martin\n",
    "- Deep Learning for Natural Language Processing by Palash Goyal, Sumit Pandey, and Karan Jain\n",
    "\n",
    "\n",
    "<B>ARTICLES</B>\n",
    "- Acheampong, Francisca Adoma; Wenyu, Chen; Nuuno-Mensah, Henry: Text‐based Emotion Detection: Advances, Challenges, and Opportunities. 2020. Engineering Reports, 2, 7. DOI: https://doi.org/10.1002/eng2.12189. [W6BG3BF5]\n",
    "- Dang, Nhan Cach; Moreno-García, Maria N.; De la Prieta, Fernando. Sentiment Analysis Based on Deep Learning: A Comparative Study.2020. Electronics, 9, 483. DOI: https://doi.org/10.3390/electronics9030483 [9M88FBQ9]\n",
    "- Blanke, Tobias; Brayant, Michel; Hedges, Mark:  Understanding Memories of the Holocaust—A New Approach to Neural Networks in the Digital Humanities. 2020. Digital Scholarship in the Humanities, 35, 1. Oxford University Press (OUP), 17–33. DOI: https://doi.org/10.1093/llc/fqy082 . [B4ETJDFD]\n",
    "- Hartmann, Jochen; Heitmann, Mark; Siebert, Christian; Schamp, Christina: More than a Feeling: Accuracy and Application of Sentiment Analysis. 2023. International Journal of Research in Marketing, 40,1, 75–87. DOI: https://doi.org/10.1016/j.ijresmar.2022.05.005. [J5E77P3M]\n",
    "- Hernández-Lorenzo, Laura; Diaz, Aitor; Perez, Alvaro; Ros, Salvador; Gonzalez-Bianco, Elena: Exploring Spanish contemporary song lyrics through Digital Humanities methods: Some thematic and structural properties. 2022. Digital Scholarship in the Humanities. 37, 3, 738–746. DOI: https://doi.org/10.1093/llc/fqab083 [4PRVFJNZ]\n",
    "- Koncar, Philipp; Geiger, Bernhard C.; Glatz, Christina; Hobisch, Elisabeth; Sarić, Sanja; Scholger, Martina; Völkl, Yvonne; Helic, Denis: A Sentiment Analysis Tool Chain for 18th Century Periodicals. 2022. In: Manuel Burghardt, Lisa Dieckmann, Timo Steyer, Peer Trilcke, Niels-Oliver Walkowski, Joëlle Weis, Ulrike Wuttke (eds.): Fabrikation von Erkenntnis. Experimente in den Digital Humanities. Luxembourg. Zeitschrift für digitale Geisteswissenschaften und Melusina Press. DOI: https://doi.org/10.26298/ezpg-wk34. [ZE7V6L5N]\n",
    "- Schmidt, Thomas; Dennerlein, Katrin; Wolff, Christian : Using Deep Learning for Emotion Analysis of 18th and 19th Century German Plays. 2021. In: Burghardt, Manuel and Dieckmann, Lisa and Steyer, Timo and Trilcke, Peer and Walkowski, Niels-Oliver and Weis, Joëlle and Wuttke, Ulrike, (eds.): Fabrikation von Erkenntnis: Experimente in den Digital Humanities. Teilband 1. Melusina Press, Esch-sur-Alzette, Luxembourg. ISBN:  978-2-919815-25-8. [RHVH9KPB]\n",
    "- Sprugnoli, Rachele; Passarotti, Marco; Cecchini, Flavio Massimiliano; Pellegrini, Matteo: Overview of the EvaLatin 2020 Evaluation Campaign. 2020. Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages (105–110). Marseille: European Language Resources Association (ELRA). [9HI22FW8]\n",
    "- Sprugnoli, Rachele;  Mambrini, Francesco; Passarotti, Marco; Moretti, Giovanni: Sentiment Analysis of Latin Poetry: First Experiments on the Odes of Horace. 2021. Proceedings of the Eighth Italian Conference on Computational Linguistics. Milan, Italy, 314–320. DOI:  https://doi.org/10.5281/zenodo.5773792. [7MNF9SMT]\n",
    "- Stanković, Ranka; Miloš Košprdić: Sentiment Analysis of Serbian Old Novels. 2022. Proceedings of the 2nd Workshop on Sentiment Analysis and Linguistic Linked Data, European Language Resources Association, 31–38. [TPHTTER9]\n",
    "- Suissa, Omri, et al. “Text Analysis Using Deep Neural Networks in Digital Humanities and Information Science.” Journal of the Association for Information Science and Technology, vol. 73, no. 2, 2022, pp. 268–87, https://doi.org/10.1002/asi.24544. [6TAVZY7M]\n",
    "- Zehe, Albin; Becker, Martin; Jannidis, Fotis; Hotho, Andreas: Towards Sentiment Analysis on German Literature. 2017. In: Kern-Isberner, G., Fürnkranz, J., Thimm, M. (eds) KI 2017: Advances in Artificial Intelligence. KI 2017. Lecture Notes in Computer Science(), 10505. Springer, Cham. DOI: https://doi.org/10.1007/978-3-319-67190-1_36. [3AHVI274]\n",
    "\n",
    "<B>TUTORIALS</B>\n",
    "- https://huggingface.co/blog/sentiment-analysis-python\n",
    "- https://towardsdatascience.com/a-step-by-step-tutorial-for-conducting-sentiment-analysis-a7190a444366\n",
    "- https://realpython.com/python-nltk-sentiment-analysis/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
